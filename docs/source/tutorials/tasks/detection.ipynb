{
  "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoof Detection Tutorial with WeDefense\n",
    "**Author:** Lin Zhang\n",
    "**Date:** 2025-10-06\n",
    "**Status:** Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook provides a step-by-step guide to running a anti-spoofing detection experiment using the WeDefense toolkit. We will follow the structure of the `run.sh` script for the `detection/asvspoof5/v03_resnet18` recipe on the PartialSpoof dataset.\n",
    "\n",
    "**Goal:** Train a model to distinguish between genuine (bonafide) and spoofed speech utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1.  **WeDefense Installation:** Ensure you have successfully installed the WeDefense toolkit and all its dependencies.\n",
    "2.  **Dataset:** This tutorial assumes you have access to the PartialSpoof dataset. The script will attempt to download it automatically if it's not found.\n",
    "3.  **Environment:** Make sure you are running this notebook from the `egs/detection/partialspoof/v03_resnet18/` directory. And installed conda enviorment success.\n",
    "4.  **Hardware:** A GPU is highly recommended for the training stage (Stage 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration\n",
    "\n",
    "First, we set up all the necessary paths and parameters for our experiment. These are the same variables you would find at the top of the `run.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Path and Data Configuration ---\n",
    "\n",
    "# TODO: IMPORTANT! Please modify this path to your PartialSpoof database directory.\n",
    "PS_dir = '/path/to/your/PartialSpoof/database'\n",
    "# Directory to store prepared data files (wav.scp, utt2lab, etc.)\n",
    "data_dir = 'data/partialspoof_tutorial'\n",
    "# The format for the dataloader. 'shard' is recommended for large datasets\n",
    "# as it groups audio files into .tar files, improving I/O efficiency.\n",
    "# 'raw' loads individual files.\n",
    "data_type = 'shard'\n",
    "\n",
    "# --- Model and Experiment Configuration ---\n",
    "# The configuration file for the model architecture and training parameters.\n",
    "config = 'conf/resnet.yaml'\n",
    "# Directory to save model checkpoints, logs, and results.\n",
    "exp_dir = 'exp/resnet_tutorial'\n",
    "\n",
    "# --- Execution Configuration ---\n",
    "# Specify which GPUs to use, e.g., \"[0]\" or \"[0,1]\".\n",
    "gpus = \"[0]\"\n",
    "# Number of models to average for inference. >0 to use averaging, <=0 to use the single best model.\n",
    "num_avg = -1\n",
    "# Save a model checkpoint every N epochs.\n",
    "save_epoch_interval = 5\n",
    "# Patience for early stopping. <0 disables it.\n",
    "early_stop_patience = -1\n",
    "# How often to run validation (in epochs).\n",
    "validate_interval = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Preparation\n",
    "\n",
    "In this stage, we process the raw PartialSpoof dataset into a standard format required by the toolkit. The `local/prepare_data.sh` script will:\n",
    "1.  Download the dataset if it's not found.\n",
    "2.  Create `wav.scp`: Maps a unique utterance ID to its audio file path. (<wav_id> \\<path> )\n",
    "3.  Create `utt2lab`: Maps each utterance ID to its label (`bonafide` or `spoof`). (<wav_id> <label>)\n",
    "4.  Create `lab2utt`: An inverted index of `utt2lab`. <Label> <wav_id>\n",
    "5.  Create `utt2dur`: Maps each utterance ID to its duration in seconds. <wav_id> <duration>\n",
    "\n",
    "This process is run in parallel for the `train`, `dev`, and `eval` sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the generated files for the train set to understand their format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1659245375.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    $ head -n 3 .data/partialspoof/train/*\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "$ head -n 3 .data/partialspoof/train/*\n",
    "# ==> ./lab2utt <==\n",
    "# spoof CON_T_0000029 CON_T_0000069 ...\n",
    "# bonafide LA_T_1138215 LA_T_1271820 ...\n",
    "\n",
    "# ==> ./utt2dur <==\n",
    "# CON_T_0000000 2.74725\n",
    "# CON_T_0000001 4.2501875\n",
    "# CON_T_0000002 3.1415\n",
    "\n",
    "# ==> ./utt2lab <==\n",
    "# CON_T_0000029 spoof\n",
    "# CON_T_0000069 spoof\n",
    "# CON_T_0000072 spoof\n",
    "\n",
    "# ==> ./wav.scp <==\n",
    "# CON_T_0000000 /export/fs05/lzhan268/workspace/PUBLIC/PartialSpoof/database/train/con_wav/CON_T_0000000.wav\n",
    "# CON_T_0000001 /export/fs05/lzhan268/workspace/PUBLIC/PartialSpoof/database/train/con_wav/CON_T_0000001.wav\n",
    "# CON_T_0000002 /export/fs05/lzhan268/workspace/PUBLIC/PartialSpoof/database/train/con_wav/CON_T_0000002.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Data Formatting and Augmentation\n",
    "\n",
    "For efficient data loading, especially in distributed training, we convert our data lists into a `shard` format. This involves bundling multiple audio files and their labels into larger `.tar` files.\n",
    "\n",
    "-   `tools/make_shard_list.py`: Creates the sharded dataset.\n",
    "-   `tools/make_raw_list.py`: Creates a simple file list (used if `data_type=\"raw\"`).\n",
    "\n",
    "We will also prepare the MUSAN (noise) and RIRS (reverberation) datasets for data augmentation during training. This step creates `wav.scp` files for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ head -n 3 .data/partialspoof/train/shard.list\n",
    "# ==> ./shard.list <==\n",
    "# data/partialspoof/train/shards/shards_000000000.tar\n",
    "# data/partialspoof/train/shards/shards_000000001.tar\n",
    "# data/partialspoof/train/shards/shards_000000002.tar\n",
    "\n",
    "# ==> ./shards <==\n",
    "# head: error reading './shards': Is a directory\n",
    "\n",
    "$ ls .data/partialspoof/train/shards\n",
    "# shards_000000000.tar shards_000000001.tar shards_000000002.tar ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Training\n",
    "\n",
    "Now we are ready to train the model. We use `torchrun` for distributed training, which is efficient even on a single machine with multiple GPUs.\n",
    "\n",
    "The training process will:\n",
    "- Load the model architecture and training parameters from the YAML config file (`conf/resnet.yaml`).\n",
    "- Use the prepared data lists (`shard.list` or `raw.list`).\n",
    "- Save model checkpoints and logs to the experiment directory (`exp/resnet_tutorial`).\n",
    "- Periodically evaluate performance on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchrun --rdzv_backend=c10d --rdzv_endpoint=$(hostname):${port} --nnodes=1 --nproc_per_node=$num_gpus \\\n",
    "    wedefense/bin/train.py --config $config \\\n",
    "      --exp_dir \"${exp_dir}\" \\\n",
    "      --gpus \"$gpus\" \\\n",
    "      --num_avg \"${num_avg}\" \\\n",
    "      --data_type \"${data_type}\" \\\n",
    "      --train_data \"${data}/train/${data_type}.list\" \\\n",
    "      --train_label \"${data}/train/utt2lab\" \\\n",
    "      --val_data \"${data}/dev/${data_type}.list\" \\\n",
    "      --val_label \"${data}/dev/utt2lab\" \\\n",
    "      --save_epoch_interval \"${save_epoch_interval}\" \\\n",
    "      --early_stop_patience \"${early_stop_patience}\" \\\n",
    "      --validate_interval \"${validate_interval}\"\n",
    "      # Add the following lines if you have prepared augmentation data\n",
    "      # --reverb_data data/rirs/lmdb \\\n",
    "      # --noise_data data/musan/lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about variables used in training please refer to \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Model Averaging & Embedding Extraction\n",
    "\n",
    "After training, we can proceed with inference. We have two options for the model to use:\n",
    "1.  **Best Model:** The single checkpoint that performed best on the development set (`best_model.pt`).\n",
    "2.  **Averaged Model:** An average of the last `num_avg` checkpoints. This often yields more robust performance.\n",
    "\n",
    "First, we average the model if `num_avg > 0`. Then, we use the chosen model to extract embeddings (fixed-size vector representations) for each utterance in the `dev` and `eval` sets. These embeddings are the input to the final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Determine which model path to use based on num_avg\n",
    "if num_avg > 0:\n",
    "    model_path = os.path.join(exp_dir, 'models/avg_model.pt')\n",
    "else:\n",
    "    model_path = os.path.join(exp_dir, 'models/best_model.pt')\n",
    "\n",
    "print(f\"Using model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 4: Model Averaging and Embedding Extraction...\"\n",
    "\n",
    "if [ ${num_avg} -gt 0 ]; then\n",
    "  echo \"Averaging the last ${num_avg} models...\"\n",
    "  python wedefense/bin/average_model.py \\\n",
    "    --dst_model \"${exp_dir}/models/avg_model.pt\" \\\n",
    "    --src_path \"${exp_dir}/models\" \\\n",
    "    --num \"${num_avg}\"\n",
    "fi\n",
    "\n",
    "echo \"Extracting embeddings...\"\n",
    "# We use a helper script for parallel embedding extraction\n",
    "local/extract_emb.sh \\\n",
    "   --exp_dir \"$exp_dir\" --model_path \"$model_path\" \\\n",
    "   --nj \"$nj\" --gpus \"$gpus\" --data_type \"$data_type\" --data \"${data}\"\n",
    "\n",
    "echo \"Stage 4 finished.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Logit Extraction\n",
    "\n",
    "With the embeddings extracted, we now pass them through the final classification layer of the model to get the raw output scores, known as **logits**. These logits represent the model's confidence for each class (`bonafide` vs. `spoof`) before any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 5: Extracting logits...\"\n",
    "\n",
    "for dset in dev eval; do\n",
    "  echo \"Processing ${dset} set...\"\n",
    "  mkdir -p \"${exp_dir}/posteriors/${dset}\"\n",
    "  python wedefense/bin/infer.py --model_path \"$model_path\" \\\n",
    "    --config \"${exp_dir}/config.yaml\" \\\n",
    "    --num_classes 2 \\\n",
    "    --embedding_scp_path \"${exp_dir}/embeddings/${dset}/embedding.scp\" \\\n",
    "    --out_path \"${exp_dir}/posteriors/${dset}\"\n",
    "done\n",
    "\n",
    "echo \"Stage 5 finished.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Score Calibration (Logits to LLR)\n",
    "\n",
    "The raw logits from the model are not always well-calibrated. To make them more interpretable and robust for decision-making, we convert them into Log-Likelihood Ratios (LLR). This process calibrates the scores based on the prior probabilities of the classes observed in the training data.\n",
    "\n",
    "A positive LLR score will indicate a prediction of 'spoof', while a negative score will indicate 'bonafide'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 6: Converting logits to Log-Likelihood Ratios (LLR)...\"\n",
    "\n",
    "# First, calculate the number of bonafide vs. spoof utterances in the training set.\n",
    "# This is used for calibration.\n",
    "cut -f2 -d\" \" \"${data}/train/utt2lab\" | sort | uniq -c | awk '{print $2 \" \" $1}' > \"${data}/train/lab2num_utts\"\n",
    "echo \"Training label counts:\"\n",
    "cat \"${data}/train/lab2num_utts\"\n",
    "\n",
    "for dset in dev eval; do\n",
    "    echo \"Calibrating scores for ${dset} set...\"\n",
    "    python wedefense/bin/logits_to_llr.py \\\n",
    "        --logits_scp_path \"${exp_dir}/posteriors/${dset}/logits.scp\" \\\n",
    "        --training_counts \"${data}/train/lab2num_utts\" \\\n",
    "        --train_label \"${data}/train/utt2lab\" \\\n",
    "        --pi_spoof 0.05 # Assumed prior probability of a spoof trial\n",
    "\n",
    "done\n",
    "\n",
    "echo \"Stage 6 finished.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: aff formula for logits to llr.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Performance Evaluation\n",
    "\n",
    "Finally, we measure the performance of our system using the calibrated LLR scores. The primary metric for anti-spoofing is the **Equal Error Rate (EER)**.\n",
    "\n",
    "- **EER:** The error rate at which the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR). A lower EER indicates better performance.\n",
    "\n",
    "We will calculate the EER for both the development and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 7: Measuring Performance...\"\n",
    "\n",
    "for dset in dev eval; do\n",
    "  echo \"Evaluating on ${dset} set...\"\n",
    "  \n",
    "  # Prepare the ground truth key file in the required format: <utt_id>\\t<label>\n",
    "  key_file=\"${data}/${dset}/cm_key_file.txt\"\n",
    "  echo -e \"filename\\tcm-label\" > \"${key_file}\"\n",
    "  # The sed command replaces the first space with a tab\n",
    "  sed 's/ /\\t/' \"${data}/${dset}/utt2lab\" >> \"${key_file}\"\n",
    "\n",
    "  # Run the evaluation script\n",
    "  # The output will be displayed here and also saved to a file in the experiment directory.\n",
    "  python wedefense/metrics/detection/evaluation.py \\\n",
    "      --m t1 \\\n",
    "      --cm \"${exp_dir}/posteriors/${dset}/llr.txt\" \\\n",
    "      --cm_key \"${key_file}\" 2>&1 | tee \"${exp_dir}/results_${dset}.txt\"\n",
    "done\n",
    "\n",
    "echo \"Stage 7 finished. Results are saved in ${exp_dir}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully completed all the stages of training and evaluating an anti-spoofing model on the PartialSpoof dataset.\n",
    "\n",
    "You have learned how to:\n",
    "- **Prepare** a dataset in the standard Kaldi-style format.\n",
    "- **Format** the data for efficient training using shards.\n",
    "- **Train** a ResNet-based model.\n",
    "- **Extract** embeddings and logits for inference.\n",
    "- **Calibrate** scores and **evaluate** the system's performance using the EER metric.\n",
    "\n",
    "### Next Steps\n",
    "- **Analyze the results:** Check the `results_eval.txt` file in your experiment directory for the final performance.\n",
    "- **Experiment with hyperparameters:** Try changing the model architecture, learning rate, or other parameters in the `conf/resnet.yaml` file.\n",
    "- **Use model averaging:** Set `num_avg` to a positive value (e.g., 5) to see if averaging checkpoints improves performance.\n",
    "- **Apply data augmentation:** If you have the MUSAN and RIRS datasets, uncomment the relevant lines in the training stage to see its effect.\n",
    "- **Embedding visualization:** You may also try to visualize the embedding extracted from the stage 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedefense_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
