{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoof Detection Tutorial with WeDefense\n",
    "**Author:** Lin Zhang\n",
    "**Date:** 2026-02-05\n",
    "**Status:** Published"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## What is Spoof Detection?\n",
    "\n",
    "Spoof detection (also known as anti-spoofing or fake audio detection) aims to detect whether an input audio sample is genuine (bonafide) or artificially generated/modified (spoofed). This is crucial for protecting automatic speaker verification systems and ensuring the authenticity of audio content.\n",
    "\n",
    "### Task Definition\n",
    "\n",
    "Given an audio input $x$, the goal is to produce a score $s$ that indicates how likely the input is genuine. \n",
    "\n",
    "<img src=\"../../_static/figures/detection.png\" width=\"300\" />\n",
    "\n",
    "In WeDefense, we model $s$ to LLR for final decision as:\n",
    "\n",
    "$$\n",
    "x \\xrightarrow{\\text{Model}} \\text{embedding} \\xrightarrow{\\text{Projection}} \\text{logits} \\xrightarrow{\\text{Calibration}} \\text{LLR score}\n",
    "$$\n",
    "\n",
    "The final LLR (Log-Likelihood Ratio) score = $\\log \\frac{p(x|H_a)}{p(x|H_r)}$ determines the decision:\n",
    "- **Positive LLR**: classified as bona fide (real)\n",
    "- **Negative LLR**: classified as spoof (fake)\n",
    "\n",
    "Where $H_a$ and $H_r$ represents the accept hypothesis (the input audio is real) and reject hypothesis (the input audio is fake), respectively. \n",
    "\n",
    "\n",
    "### Why use LLR instead of raw posterior from network?\n",
    "\n",
    "While spoof detection can be framed as binary classification, we recommend using calibrated LLR scores instead of raw logits or posteriors for several reasons:\n",
    "\n",
    "1. **Prior-aware calibration**: LLR incorporates the prior probability of spoofing attacks, making it more suitable for real-world deployment where attack frequencies vary.\n",
    "2. **Interpretability**: LLR provides a principled decision threshold (0) with clear probabilistic meaning that $p(x|H_a) = p(x|H_r)$.\n",
    "3. **Robustness**: Calibrated scores are less sensitive to training data imbalance and generalize better across datasets.\n",
    "\n",
    "Note,\n",
    "- Genuine/Bona fide/Real: Speech spoken by human or authentic audio naturally captured from real sources.\n",
    "- Spoof/Fake: Generated or modified audio by machine(e.g., TTS, voice conversion, etc.)\n",
    "\n",
    "### Evaluation Metrics\n",
    "WeDefense follows [ASVspoof5](https://www.asvspoof.org/) (appendix of [the eval. plan](https://www.asvspoof.org/file/ASVspoof5___Evaluation_Plan_Phase2.pdf)) official evaluation metrics. Lower is better for all of them.\n",
    "\n",
    "\n",
    "| Metric | Range | What it measures |\n",
    "|--------|------|-----------------|\n",
    "| **min-DCF** | [0, $\\infty$] | Best achievable cost (optimal threshold) | \n",
    "| **EER** | [0, 0.5] | Equal error rate where false negative rate and false positive rate are equal (threshold-independent) | \n",
    "| **$C_\\text{llr}$** | [0, $\\infty$] | Score calibration quality. 0 for perfect calibration, 1 for non-informative system, and $\\gt 1$ for worse than non-informative | \n",
    "| **act-DCF** | [0, $\\infty$] | Detection cost at a fixed Bayesian threshold, sensitive to calibration. (So act-DCF $\\ge$ min-DCF.) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation Metrics\n",
    "WeDefense follows [ASVspoof5](https://www.asvspoof.org/) official evaluation metrics:\n",
    "> Delgado, H., Evans, N., Jung, J.W., Kinnunen, T., Kukanov, I., Lee, K.A., Liu, X., Shim, H.J., Sahidullah, M., Tak, H. and Todisco, M., 2024. ASVspoof 5 evaluation plan.\n",
    "\n",
    "**Default parameters**:\n",
    "- $P_\\text{spoof} = 0.05$ (prior probability of spoofing attack)\n",
    "- $C_\\text{miss} = 1$ (cost of falsely rejecting bonafide)\n",
    "- $C_\\text{fa} = 10$ (cost of falsely accepting spoof)\n",
    "\n",
    "### 1. Minimum Detection Cost Function (min-DCF) - **Primary Metric**\n",
    "**Definition**: The minimum normalized detection cost over all possible thresholds.\n",
    "\n",
    "$$\n",
    "\\text{min-DCF} = \\frac{\\min_\\theta C(\\theta)}{C_\\text{default}}\n",
    "$$\n",
    "\n",
    "Where the unnormalized cost is:\n",
    "$$\n",
    "C(\\theta) = C_\\text{miss} \\cdot P_\\text{miss}(\\theta) \\cdot (1-P_\\text{spoof}) + C_\\text{fa} \\cdot P_\\text{fa}(\\theta) \\cdot P_\\text{spoof}\n",
    "$$\n",
    "\n",
    "And the default cost (normalization factor):\n",
    "$$\n",
    "C_\\text{default} = \\min(C_\\text{miss} \\cdot (1-P_\\text{spoof}), C_\\text{fa} \\cdot P_\\text{spoof})\n",
    "$$\n",
    "\n",
    "- **Range**: [0, 1], **Lower is better**\n",
    "- **Pros**: Application-aware, considers different error costs and priors\n",
    "- **Cons**: Optimistic (assumes optimal threshold selection)\n",
    "\n",
    "### 2. Equal Error Rate (EER) - **Secondary Metric**\n",
    "**Definition**: The error rate at which False Rejection Rate (FRR) equals False Acceptance Rate (FAR).\n",
    "\n",
    "$$\n",
    "\\text{EER} = \\text{FRR}(\\theta^*) = \\text{FAR}(\\theta^*) \\text{ where } \\theta^* = \\arg\\min_\\theta |\\text{FRR}(\\theta) - \\text{FAR}(\\theta)|\n",
    "$$\n",
    "\n",
    "- **Range**: [0, 1] or [0%, 100%], **Lower is better**\n",
    "- **Pros**: Simple, intuitive, threshold-independent\n",
    "- **Cons**: Doesn't reflect application costs; assumes equal cost for errors\n",
    "\n",
    "### 3. Log-Likelihood Ratio Cost ($C_\\text{llr}$) - **Secondary Metric**\n",
    "> Brümmer, N. and Du Preez, J., 2006. Application-independent evaluation of speaker detection. Computer Speech & Language, 20(2-3), pp.230-275.\n",
    "\n",
    "**Definition**: Measures the quality of calibrated LLR scores (discrimination + calibration). Recommended by ASVspoof committee from 2024.\n",
    "\n",
    "$$\n",
    "C_\\text{llr} = \\frac{1}{2}\\left[\\frac{1}{N_\\text{bon}}\\sum_{i \\in \\text{bonafide}} \\log_2(1 + e^{-s_i}) + \\frac{1}{N_\\text{spf}}\\sum_{j \\in \\text{spoof}} \\log_2(1 + e^{s_j})\\right]\n",
    "$$\n",
    "\n",
    "Where $s$ is the LLR score.\n",
    "\n",
    "- **Range**: [0, ∞), typical values [0, 2], **Lower is better**\n",
    "- **Pros**: Evaluates both discrimination and calibration; proper scoring rule\n",
    "- **Cons**: Less interpretable than error rates; requires well-calibrated LLR\n",
    "\n",
    "### 4. Actual Detection Cost Function (act-DCF) - **Secondary Metric**\n",
    "**Definition**: The detection cost at a Bayes-optimal decision threshold (fixed by prior and costs).\n",
    "\n",
    "$$\n",
    "\\text{act-DCF} = \\frac{C(\\theta_\\text{Bayes})}{C_\\text{default}}\n",
    "$$\n",
    "\n",
    "The Bayes-optimal threshold is determined by the cost ratio:\n",
    "$$\n",
    "\\theta_\\text{Bayes} = -\\log\\left(\\frac{C_\\text{miss} \\cdot (1-P_\\text{spoof})}{C_\\text{fa} \\cdot P_\\text{spoof}}\\right)\n",
    "$$\n",
    "\n",
    "With default parameters: $\\theta_\\text{Bayes} = -\\log(19) \\approx -2.944$\n",
    "\n",
    "- **Range**: [0, ∞), typically [0, 2], **Lower is better**\n",
    "- **Pros**: Reflects real deployment with fixed threshold; realistic performance measure\n",
    "- **Cons**: Sensitive to threshold calibration; $\\text{act-DCF} \\geq \\text{min-DCF}$\n",
    "\n",
    "**Note**: Whereas min-DCF measures performance using an _oracle_ threshold (which can only be set when ground-truth is known), act-DCF measures the realized cost obtained by setting the threshold to $\\theta_\\text{Bayes}$ without seeing the test labels.\n",
    "\n",
    "### Metric Comparison\n",
    "\n",
    "| Metric | Type | What it measures | Best for | Limitation |\n",
    "|--------|------|-----------------|----------|-----------|\n",
    "| **min-DCF** | **Primary** | Best achievable cost (optimal threshold) | **System ranking** | Optimistic assumption |\n",
    "| **EER** | Secondary | Balanced error rate | Quick comparison | Ignores costs & priors |\n",
    "| **$C_\\text{llr}$** | Secondary | Score calibration quality | **LLR reliability** | Requires calibrated scores |\n",
    "| **act-DCF** | Secondary | Real-world cost (fixed threshold) | **Deployment readiness** | Depends on calibration |\n",
    "\n",
    "**ASVspoof5 Evaluation Strategy**: \n",
    "- **Primary metric**: min-DCF (for official system ranking)\n",
    "- **Secondary metrics**: EER, $C_\\text{llr}$, act-DCF (for comprehensive analysis)\n",
    "- Systems are primarily ranked by **min-DCF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step-by-Step Implementation in WeDefense\n",
    "\n",
    "The implementation stages in WeDefense are:\n",
    "\n",
    "- **Stage 1–2:** Data preparation and list generation.\n",
    "- **Stage 3:** Model training.\n",
    "- **Stage 4:** Model averaging and embedding extraction.\n",
    "- **Stage 5:** Logit extraction (and optional posterior output via softmax).\n",
    "- **Stage 6:** Score calibration (logits to LLR).\n",
    "- **Stage 7:** Performance evaluation.\n",
    "\n",
    "**Note:** \n",
    "1. WeDefense separates embedding extraction (Stage 4) from logit/posterior prediction (Stage 5) to keep the pipeline modular. This makes it easier to analyze or visualize embeddings, reuse the same embeddings with different back-end scoring methods, and debug each stage independently. \n",
    "2. For evaluation, we convert logits to LLR (Stage 6) to apply prior-aware calibration and obtain well-calibrated decision scores rather than using raw logits or posteriors directly.\n",
    "\n",
    "In the following of this notebook, we provide a step-by-step guide to running a anti-spoofing detection experiment using the WeDefense toolkit. We will follow the structure of the `run.sh` script for the `detection/asvspoof5/v03_resnet18` recipe on the PartialSpoof dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1.  **WeDefense Installation:** Ensure you have successfully installed the WeDefense toolkit and all its dependencies.\n",
    "2.  **Dataset:** This tutorial assumes you have access to the PartialSpoof dataset. The script will attempt to download it automatically if it's not found.\n",
    "3.  **Environment:** Make sure you are running this notebook from the `egs/detection/partialspoof/v03_resnet18/` directory. And installed conda enviorment success.\n",
    "4.  **Hardware:** A GPU is highly recommended for the training stage (Stage 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration\n",
    "\n",
    "First, we set up all the necessary paths and parameters for our experiment. These are the same variables you would find at the top of the `run.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Path and Data Configuration ---\n",
    "\n",
    "# TODO: IMPORTANT! Please modify this path to your PartialSpoof database directory.\n",
    "PS_dir = '/path/to/your/PartialSpoof/database'\n",
    "# Directory to store prepared data files (wav.scp, utt2lab, etc.)\n",
    "data_dir = 'data/partialspoof_tutorial'\n",
    "# The format for the dataloader. 'shard' is recommended for large datasets\n",
    "# as it groups audio files into .tar files, improving I/O efficiency.\n",
    "# 'raw' loads individual files.\n",
    "data_type = 'shard'\n",
    "\n",
    "# --- Model and Experiment Configuration ---\n",
    "# The configuration file for the model architecture and training parameters.\n",
    "config = 'conf/resnet.yaml'\n",
    "# Directory to save model checkpoints, logs, and results.\n",
    "exp_dir = 'exp/resnet_tutorial'\n",
    "\n",
    "# --- Execution Configuration ---\n",
    "# Specify which GPUs to use, e.g., \"[0]\" or \"[0,1]\".\n",
    "gpus = \"[0]\"\n",
    "# Number of models to average for inference. >0 to use averaging, <=0 to use the single best model.\n",
    "num_avg = -1\n",
    "# Save a model checkpoint every N epochs.\n",
    "save_epoch_interval = 5\n",
    "# Patience for early stopping. <0 disables it.\n",
    "early_stop_patience = -1\n",
    "# How often to run validation (in epochs).\n",
    "validate_interval = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Preparation\n",
    "\n",
    "In this stage, we process the raw PartialSpoof dataset into a standard format required by the toolkit. The `local/prepare_data.sh` script will:\n",
    "1.  Download the dataset if it's not found.\n",
    "2.  Create `wav.scp`: Maps a unique utterance ID to its audio file path. (<wav_id> \\<path> )\n",
    "3.  Create `utt2lab`: Maps each utterance ID to its label (`bonafide` or `spoof`). (<wav_id> <label>)\n",
    "4.  Create `lab2utt`: An inverted index of `utt2lab`. <Label> <wav_id>\n",
    "5.  Create `utt2dur`: Maps each utterance ID to its duration in seconds. <wav_id> <duration>\n",
    "\n",
    "This process is run in parallel for the `train`, `dev`, and `eval` sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the generated files for the train set to understand their format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ head -n 3 .data/partialspoof/train/*\n",
    "# ==> ./lab2utt <==\n",
    "# spoof CON_T_0000029 CON_T_0000069 ...\n",
    "# bonafide LA_T_1138215 LA_T_1271820 ...\n",
    "\n",
    "# ==> ./utt2dur <==\n",
    "# CON_T_0000000 2.74725\n",
    "# CON_T_0000001 4.2501875\n",
    "# CON_T_0000002 3.1415\n",
    "\n",
    "# ==> ./utt2lab <==\n",
    "# CON_T_0000029 spoof\n",
    "# CON_T_0000069 spoof\n",
    "# CON_T_0000072 spoof\n",
    "\n",
    "# ==> ./wav.scp <==\n",
    "# CON_T_0000000 /export/fs05/lzhan268/workspace/PUBLIC/PartialSpoof/database/train/con_wav/CON_T_0000000.wav\n",
    "# CON_T_0000001 /export/fs05/lzhan268/workspace/PUBLIC/PartialSpoof/database/train/con_wav/CON_T_0000001.wav\n",
    "# CON_T_0000002 /export/fs05/lzhan268/workspace/PUBLIC/PartialSpoof/database/train/con_wav/CON_T_0000002.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Data Formatting and Augmentation\n",
    "\n",
    "For efficient data loading, especially in distributed training, we convert our data lists into a `shard` format. This involves bundling multiple audio files and their labels into larger `.tar` files.\n",
    "\n",
    "-   `tools/make_shard_list.py`: Creates the sharded dataset.\n",
    "-   `tools/make_raw_list.py`: Creates a simple file list (used if `data_type=\"raw\"`).\n",
    "\n",
    "We will also prepare the MUSAN (noise) and RIRS (reverberation) datasets for data augmentation during training. This step creates `wav.scp` files for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ head -n 3 .data/partialspoof/train/shard.list\n",
    "# ==> ./shard.list <==\n",
    "# data/partialspoof/train/shards/shards_000000000.tar\n",
    "# data/partialspoof/train/shards/shards_000000001.tar\n",
    "# data/partialspoof/train/shards/shards_000000002.tar\n",
    "\n",
    "# ==> ./shards <==\n",
    "# head: error reading './shards': Is a directory\n",
    "\n",
    "$ ls .data/partialspoof/train/shards\n",
    "# shards_000000000.tar shards_000000001.tar shards_000000002.tar ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Training\n",
    "\n",
    "Now we are ready to train the model. We use `torchrun` for distributed training, which is efficient even on a single machine with multiple GPUs.\n",
    "\n",
    "The training process will:\n",
    "- Load the model architecture and training parameters from the YAML config file (`conf/resnet.yaml`).\n",
    "- Use the prepared data lists (`shard.list` or `raw.list`).\n",
    "- Save model checkpoints and logs to the experiment directory (`exp/resnet_tutorial`).\n",
    "- Periodically evaluate performance on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchrun --rdzv_backend=c10d --rdzv_endpoint=$(hostname):${port} --nnodes=1 --nproc_per_node=$num_gpus \\\n",
    "    wedefense/bin/train.py --config $config \\\n",
    "      --exp_dir \"${exp_dir}\" \\\n",
    "      --gpus \"$gpus\" \\\n",
    "      --num_avg \"${num_avg}\" \\\n",
    "      --data_type \"${data_type}\" \\\n",
    "      --train_data \"${data}/train/${data_type}.list\" \\\n",
    "      --train_label \"${data}/train/utt2lab\" \\\n",
    "      --val_data \"${data}/dev/${data_type}.list\" \\\n",
    "      --val_label \"${data}/dev/utt2lab\" \\\n",
    "      --save_epoch_interval \"${save_epoch_interval}\" \\\n",
    "      --early_stop_patience \"${early_stop_patience}\" \\\n",
    "      --validate_interval \"${validate_interval}\"\n",
    "      # Add the following lines if you have prepared augmentation data\n",
    "      # --reverb_data data/rirs/lmdb \\\n",
    "      # --noise_data data/musan/lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about variables used in training please refer to \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Model Averaging & Embedding Extraction\n",
    "\n",
    "After training, we can proceed with inference. We have two options for the model to use:\n",
    "1.  **Best Model:** The single checkpoint that performed best on the development set (`best_model.pt`).\n",
    "2.  **Averaged Model:** An average of the last `num_avg` checkpoints. This often yields more robust performance.\n",
    "\n",
    "First, we average the model if `num_avg > 0`. Then, we use the chosen model to extract embeddings (fixed-size vector representations) for each utterance in the `dev` and `eval` sets. These embeddings are the input to the final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Determine which model path to use based on num_avg\n",
    "if num_avg > 0:\n",
    "    model_path = os.path.join(exp_dir, 'models/avg_model.pt')\n",
    "else:\n",
    "    model_path = os.path.join(exp_dir, 'models/best_model.pt')\n",
    "\n",
    "print(f\"Using model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 4: Model Averaging and Embedding Extraction...\"\n",
    "\n",
    "if [ ${num_avg} -gt 0 ]; then\n",
    "  echo \"Averaging the last ${num_avg} models...\"\n",
    "  python wedefense/bin/average_model.py \\\n",
    "    --dst_model \"${exp_dir}/models/avg_model.pt\" \\\n",
    "    --src_path \"${exp_dir}/models\" \\\n",
    "    --num \"${num_avg}\"\n",
    "fi\n",
    "\n",
    "echo \"Extracting embeddings...\"\n",
    "# We use a helper script for parallel embedding extraction\n",
    "local/extract_emb.sh \\\n",
    "   --exp_dir \"$exp_dir\" --model_path \"$model_path\" \\\n",
    "   --nj \"$nj\" --gpus \"$gpus\" --data_type \"$data_type\" --data \"${data}\"\n",
    "\n",
    "echo \"Stage 4 finished.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Logit Extraction\n",
    "\n",
    "With the embeddings extracted, we now pass them through the final classification layer of the model to get the raw output scores, known as **logits**. These logits represent the model's confidence for each class (`bonafide` vs. `spoof`) before any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 5: Extracting logits...\"\n",
    "\n",
    "for dset in dev eval; do\n",
    "  echo \"Processing ${dset} set...\"\n",
    "  mkdir -p \"${exp_dir}/posteriors/${dset}\"\n",
    "  python wedefense/bin/infer.py --model_path \"$model_path\" \\\n",
    "    --config \"${exp_dir}/config.yaml\" \\\n",
    "    --num_classes 2 \\\n",
    "    --embedding_scp_path \"${exp_dir}/embeddings/${dset}/embedding.scp\" \\\n",
    "    --out_path \"${exp_dir}/posteriors/${dset}\"\n",
    "done\n",
    "\n",
    "echo \"Stage 5 finished.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Score Calibration (Logits to LLR)\n",
    "\n",
    "The raw logits from the model are not always well-calibrated. To make them more interpretable and robust for decision-making, we convert them into Log-Likelihood Ratios (LLR). This process calibrates the scores based on the prior probabilities of the classes observed in the training data.\n",
    "\n",
    "A positive LLR score will indicate a prediction of 'bonafide', while a negative score will indicate 'spoof'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 6: Converting logits to Log-Likelihood Ratios (LLR)...\"\n",
    "\n",
    "# First, calculate the number of bonafide vs. spoof utterances in the training set.\n",
    "# This is used for calibration.\n",
    "cut -f2 -d\" \" \"${data}/train/utt2lab\" | sort | uniq -c | awk '{print $2 \" \" $1}' > \"${data}/train/lab2num_utts\"\n",
    "echo \"Training label counts:\"\n",
    "cat \"${data}/train/lab2num_utts\"\n",
    "\n",
    "for dset in dev eval; do\n",
    "    echo \"Calibrating scores for ${dset} set...\"\n",
    "    python wedefense/bin/logits_to_llr.py \\\n",
    "        --logits_scp_path \"${exp_dir}/posteriors/${dset}/logits.scp\" \\\n",
    "        --training_counts \"${data}/train/lab2num_utts\" \\\n",
    "        --train_label \"${data}/train/utt2lab\" \\\n",
    "        --pi_spoof 0.05 # Assumed prior probability of a spoof trial\n",
    "\n",
    "done\n",
    "\n",
    "echo \"Stage 6 finished.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "**Logits → LLR (binary case)**\n",
    "\n",
    "Let the model output logits $s_{\\text{spoof}}$ and $s_{\\text{bonafide}}$. The posterior is\n",
    "\n",
    "$$\n",
    "P(\\text{spoof}\\mid x)=\\frac{e^{s_{\\text{spoof}}}}{e^{s_{\\text{spoof}}}+e^{s_{\\text{bonafide}}}},\\quad\n",
    "P(\\text{bonafide}\\mid x)=\\frac{e^{s_{\\text{bonafide}}}}{e^{s_{\\text{spoof}}}+e^{s_{\\text{bonafide}}}}\n",
    "$$\n",
    "\n",
    "The log-likelihood ratio is\n",
    "\n",
    "$$\n",
    "\\text{LLR}(x)=\\log\\frac{P(\\text{spoof}\\mid x)}{P(\\text{bonafide}\\mid x)}-\\log\\frac{\\pi_{\\text{spoof}}}{1-\\pi_{\\text{spoof}}}\n",
    "$$\n",
    "\n",
    "where $\\pi_{\\text{spoof}}$ is the prior spoof probability used for calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Performance Evaluation\n",
    "\n",
    "Finally, we measure the performance of our system using the calibrated LLR scores. The primary metric for anti-spoofing is **min-DCF**, with **EER** reported as a secondary metric.\n",
    "\n",
    "- **min-DCF:** The minimum normalized detection cost over all possible thresholds. Lower is better.\n",
    "- **EER:** The error rate at which the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR). Lower is better.\n",
    "\n",
    "We will calculate min-DCF and EER for both the development and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"Starting Stage 7: Measuring Performance...\"\n",
    "\n",
    "for dset in dev eval; do\n",
    "  echo \"Evaluating on ${dset} set...\"\n",
    "  \n",
    "  # Prepare the ground truth key file in the required format: <utt_id>\\t<label>\n",
    "  key_file=\"${data}/${dset}/cm_key_file.txt\"\n",
    "  echo -e \"filename\\tcm-label\" > \"${key_file}\"\n",
    "  # The sed command replaces the first space with a tab\n",
    "  sed 's/ /\\t/' \"${data}/${dset}/utt2lab\" >> \"${key_file}\"\n",
    "\n",
    "  # Run the evaluation script\n",
    "  # The output will be displayed here and also saved to a file in the experiment directory.\n",
    "  python wedefense/metrics/detection/evaluation.py \\\n",
    "      --m t1 \\\n",
    "      --cm \"${exp_dir}/posteriors/${dset}/llr.txt\" \\\n",
    "      --cm_key \"${key_file}\" 2>&1 | tee \"${exp_dir}/results_${dset}.txt\"\n",
    "done\n",
    "\n",
    "echo \"Stage 7 finished. Results are saved in ${exp_dir}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully completed all the stages of training and evaluating an anti-spoofing model on the PartialSpoof dataset.\n",
    "\n",
    "### Next Steps\n",
    "- **Analyze the results:** Check the `results_eval.txt` file in your experiment directory for the final performance.\n",
    "- **Experiment with hyperparameters:** Try changing the model architecture, learning rate, or other parameters in the `conf/resnet.yaml` file.\n",
    "- **Embedding visualization:** You may also try to visualize the embedding extracted from the stage 4 following `wedefense/egs/embedding_visualization/embedding_visulization_umap.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedefense_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
